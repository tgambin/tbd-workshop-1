{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAC Lab 3 - MLFlow\n",
    "\n",
    "Your task will be to:\n",
    "1. run MLFlow server\n",
    "2. create and perform feature engineering pipeline of Microsoft Security Incident Prediction using Apache Spark\n",
    "3. create ML model and register it to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/mlflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run MLFlow server and expose it using ngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mount the Google Drive filesystem in Colab and changes the current working directory to a specific directory within the mounted Google Drive directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount to Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# first you need to create a folders' path in your drive called learn/mlflow\n",
    "os.chdir('/content/drive/My Drive/learn/mlflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second cell installs the MLflow Python package if it's not already installed, and then imports it. It also imports several other Python packages that may be used later in the code, such as os and pandas. Finally, it prints the version of MLflow that is installed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Installing MLflow and checking the version\n",
    "\n",
    "# install and import mlflow\n",
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec('mlflow') is None:\n",
    "  !pip install mlflow --q\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a directory called artefacts_mlflow if it doesn't already exist, and then creates an MLflow experiment with the name \"Iris Classification\". It then retrieves the ID of the newly created experiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Setting mlflow artefacts\n",
    "artefacts_temp_dir = 'artefacts_mlflow'\n",
    "if not os.path.exists(artefacts_temp_dir):\n",
    "    os.makedirs(artefacts_temp_dir)\n",
    "\n",
    "mlflow.create_experiment('Iris Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the experiment ID for the experiment with the specified name\n",
    "experiment_id = mlflow.get_experiment_by_name('Iris Classification').experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2 - Starting MLflow, running UI in background\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"my-run\", nested=True, experiment_id=experiment_id):\n",
    "    # Log some metrics\n",
    "    mlflow.log_metric(\"accuracy\", 0.85)\n",
    "    mlflow.log_metric(\"precision\", 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth cell starts a new MLflow run within the previously created experiment, with the name \"my-run\". It then logs two metrics for the run, \"accuracy\" and \"precision\", with the respective values 0.85 and 0.75. Finally, it starts the MLflow tracking UI in the background using a system command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install the Pyngrok Python package and imports it, and then prompt the user to enter their Ngrok authentication token. It then sets the authentication token in the Pyngrok library, creates an HTTP tunnel to the MLflow tracking UI running on port 5000, and prints the public URL of the tunnel. This allows the user to access the MLflow tracking UI from a remote location.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://hackernoon.com/hn-images/1*OBNbvLxAESaQTEqWdqBCGw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tracking UI in the background\n",
    "get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
    "## Step 3 - Installing pyngrok for remote tunnel access using ngrock.com\n",
    "!pip install pyngrok --quiet\n",
    "from pyngrok import ngrok\n",
    "from getpass import getpass\n",
    "# Terminate open tunnels if any exist\n",
    "ngrok.kill()\n",
    "## Step 4 - Login on ngrok.com and get your authtoken from https://dashboard.ngrok.com\n",
    "# Enter your auth token when the code is running\n",
    "NGROK_AUTH_TOKEN = getpass('Enter the ngrok authtoken: ')\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
    "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EXAMPLE - Feature engineering and model learning using Apache Spark with register to MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intasll packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U  google.cloud \"pandas<2.0.0\" google-cloud-storage==2.9.0 mlflow==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change MLFLOW_TRACKING_URI to your URI from ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLFLOW_TRACKING_URI=https://ea34-34-21-9-167.ngrok-free.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test connectivity with MLflow tracking server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mlflow experiments search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_path = \"gs://gdl-workshops-bd-public\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data for example and your implementation are located in GCS bucket `bucket_path`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCS connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O /tmp/gcs-connector-hadoop3-2.2.17-shaded.jar  https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.17/gcs-connector-hadoop3-2.2.17-shaded.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/gcs-connector-hadoop3-2.2.17-shaded.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.null.enable\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"tbd\"\n",
    "gs_path = \"gs://gdl-workshops-bd-public/survey_results_public.csv\"\n",
    "spark.sql(f'DROP DATABASE IF EXISTS {db_name} CASCADE')\n",
    "spark.sql(f'CREATE DATABASE {db_name}')\n",
    "spark.sql(f'USE {db_name}')\n",
    "table_name = \"survey_2020\" \n",
    "\n",
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')\n",
    "\n",
    "spark_df= spark.sql(f'SELECT *, CAST((ConvertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} WHERE ConvertedComp IS NOT NULL ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "y = 'compAboveAvg' \n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'YearsCode']\n",
    "\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='strindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label').setHandleInvalid(\"keep\")]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='strindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]\n",
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') \n",
    "\n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "\n",
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234) # Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the experiment that we would like to use for tracking training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow   \n",
    "import mlflow.spark\n",
    "\n",
    "ename = f\"my_username\"\n",
    "artifacts_location= \"artifacts\"\n",
    "mlflow.set_experiment(experiment_name=ename)\n",
    "experiment = mlflow.get_experiment_by_name(ename)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare metrics that we would like to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "evaluator_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "evaluator_f = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a training using the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id):\n",
    "    mlflow.log_param('model_type', 'DecisionTreeClassifier')\n",
    "    dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "    mlflow.set_tag(\"classifier\", \"decision_tree\")  ## ustawienie tagow\n",
    "    mlflow.log_param(\"depth\", dt.getMaxDepth())    ## zapisanie metadanych - hiperparametrow\n",
    "\n",
    "    dt_model = Pipeline(stages=[dt]).fit(training)\n",
    "    pred_dt = dt_model.transform(test)\n",
    "    label_and_pred = pred_dt.select('label', 'prediction')\n",
    "    res = dt_model.transform(test)\n",
    "\n",
    "    test_metric_acc = evaluator_acc.evaluate(res)\n",
    "    test_metric_recall = evaluator_recall.evaluate(res)\n",
    "    test_metric_prec = evaluator_prec.evaluate(res)\n",
    "    test_metric_f = evaluator_f.evaluate(res)\n",
    "\n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f)\n",
    "    mlflow.spark.log_model(dt_model, artifact_path=artifacts_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a training using the gradient boost trees model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "gbt_model = gbt.fit(training)\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id):\n",
    "    mlflow.log_param('model_type', 'GBTClassifier')\n",
    "    mlflow.log_param(\"depth\", gbt.getMaxDepth())\n",
    "    res = gbt_model.transform(test)\n",
    "    test_metric_acc = evaluator_acc.evaluate(res)\n",
    "    test_metric_recall = evaluator_recall.evaluate(res)\n",
    "    test_metric_prec = evaluator_prec.evaluate(res)\n",
    "    test_metric_f = evaluator_f.evaluate(res)\n",
    "\n",
    "    mlflow.log_metric(evaluator_acc.getMetricName(), test_metric_acc) \n",
    "    mlflow.log_metric(evaluator_recall.getMetricName(), test_metric_recall) \n",
    "    mlflow.log_metric(evaluator_prec.getMetricName(), test_metric_prec)     \n",
    "    mlflow.log_metric(evaluator_f.getMetricName(), test_metric_f) \n",
    "  \n",
    "    mlflow.spark.log_model(spark_model=gbt_model, artifact_path='gbt_classifier') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/83b4e502895840719d976337812b0d3b/artifacts'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "prediction = loaded_model.predict(pd.DataFrame(test.limit(10).select('features').toPandas()))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Student implementation\n",
    "- Please prepare machine learning pipeline (feature engineering and machine learning using appropriate model) for Microsoft Security Incident Prediction from Kaggle, using Apache Spark, following the example.\n",
    "- The model should be registered on your MLFlow with appropriate metrics.\n",
    "- Please provide screenshots of MLFlow dashboard, export this notebook as PDF and provide in the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A. Data Preprocessing\n",
    "\n",
    "Please perform data preprocessing using Apache Spark, including the following steps:\n",
    "- Handle missing and incomplete data: Identify and appropriately address null or missing values, either by removing rows/columns, imputing values, or other relevant techniques.\n",
    "- Select relevant features for prediction: Remove columns that do not contribute to predicting the label, such as unique identifiers or irrelevant metadata.\n",
    "- Split the dataset: Divide the dataset into training, test, and optionally validation sets in appropriate proportions (e.g., 70% training, 20% test, 10% validation).\n",
    "- Normalize the data: Apply feature scaling to ensure the input features are on a similar scale, which is crucial for many machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B. Model Training, Testing, and Deployment Using Spark and MLflow\n",
    "\n",
    "Please perform the training of machine learning models using Apache Spark and MLflow, with the following three classifiers:\n",
    "- SVC (Support Vector Classifier)\n",
    "- MLP (Multi-Layer Perceptron)\n",
    "- KNN (k-nearest neighbors).\n",
    "\n",
    "Please classify column called `IncidentGrade`.\n",
    "\n",
    "Tasks:\n",
    "1.\tTrain the models using the training dataset.\n",
    "2.\tTest the models using the test dataset.\n",
    "3.\tEvaluate and compare model performance using multiple metrics such as:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "4. Track and log experiments using MLflow, including:\n",
    "- Parameters\n",
    "- Metrics\n",
    "- Model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
